{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94072413-0356-46bd-87cb-3e8750197c78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19be2d60-38c2-4976-afc3-b2847c12f0de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS pyspark_cata.source.customers \n",
    "(\n",
    "  id STRING,\n",
    "  email STRING,\n",
    "  city STRING,\n",
    "  country STRING,\n",
    "  ModifyDate TIMESTAMP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "829ea4f3-c5b0-4f5f-831d-e5175fabdd68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from pyspark_cata.source.customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb195293-96d6-4449-860b-9c5cb2c9357f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "INSERT INTO pyspark_cata.source.customers\n",
    "VALUES ('1','eklavya@databricks.com','patna','India', current_timestamp()),\n",
    "('2','john.smith@databricks.com','jersey','USA',current_timestamp()),\n",
    "('3','jane.doe@databricks.com','madrid','Spain',current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2544211c-d084-4c39-aa7a-704e7b656802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM pyspark_cata.source.customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7667dafe-3258-486e-83ba-dcf2a66a6bf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "df  = spark.read.table(\"pyspark_cata.source.customers\")\n",
    "df = df.withColumn(\"startDate\",current_timestamp())\\\n",
    "    .withColumn(\"endDate\",to_timestamp(lit(\"31-12-9999\"),'dd-mm-yyyy'))\\\n",
    "        .withColumn(\"active\",lit(True)).withColumn(\"id\",col(\"id\").cast(\"int\"))\n",
    "\n",
    "w = Window.partitionBy(\"id\").orderBy(col(\"ModifyDate\").desc())\n",
    "df = df.withColumn(\"rn\",row_number().over(w))\n",
    "df = df.filter(col(\"rn\")==1).drop(\"rn\")\n",
    "df.createOrReplaceTempView(\"src\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f1019dc-e813-4828-a6ce-0be99ff88e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "if spark.catalog.tableExists('pyspark_cata.source.dimCustomers'):\n",
    "    spark.sql(\"\"\"\n",
    "              MERGE INTO pyspark_cata.source.dimCustomers tgt\n",
    "              USING src\n",
    "              ON tgt.id = src.id AND tgt.active = True\n",
    "              WHEN MATCHED AND tgt.city <> src.city \n",
    "              OR tgt.country<> src.country \n",
    "              OR tgt.email<> src.email \n",
    "              OR tgt.ModifyDate <> src.ModifyDate \n",
    "              THEN\n",
    "              UPDATE \n",
    "              SET tgt.endDate = current_timestamp(),\n",
    "              tgt.active = False\n",
    "              \"\"\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "              MERGE INTO pyspark_cata.source.dimCustomers tgt\n",
    "              USING src\n",
    "              ON tgt.id = src.id AND tgt.act ive = True\n",
    "\n",
    "              WHEN NOT MATCHED THEN INSERT *\n",
    "              \"\"\")\n",
    "\n",
    "     \n",
    "else:\n",
    "    # df = spark.read.table(\"pyspark_cata.source.customers\")\n",
    "    # df = df.withColumn(\"startDate\",current_timestamp())\\\n",
    "    #     .withColumn(\"endDate\",to_timestamp(lit(\"31-12-9999\"),'dd-mm-yyyy'))\\\n",
    "    #         .withColumn(\"acive\",lit(True)).withColumn(\"id\",col(\"id\").cast(\"int\"))\n",
    "\n",
    "    #doing the same thing dome above in sql below using a temp view made on source df\n",
    "     \n",
    "    #spark.sql(\"select *,current_timestamp() as startDate,CAST(\"31-12-9999\" as TIMESTAMP) as endDate\n",
    "    #,True as active from srctmp\")\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"pyspark_cata.source.dimCustomers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a169c1-3953-4928-9b82-051360539b2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"pyspark_cata.source.dimCustomers\")\n",
    "df = df.drop(\"acive\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",True).saveAsTable(\"pyspark_cata.source.dimCustomers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50eda967-e723-48d0-a228-d640f00ad0c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from pyspark_cata.source.dimCustomers\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8885071629717031,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
